---
title: "F-Tests and Qualitative Predictors"
author: "John Mayberry"
date: "01-31-2025"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE,message=FALSE)
knitr::opts_chunk$set(echo = TRUE)
url="https://www.statlearning.com/s/Advertising.csv"
Advertising=read_csv(url) %>% select(c("sales","TV","radio","newspaper"))

```

## Introduction

In these notes, we will be explaining the last line of the regression output

```{r}
sales_lmINT=lm(sales~TV*radio,data=Advertising)
summary(sales_lmINT)
```

In particular, we'll discuss two important issues with multiple regression:

1. How do we know that our model is significantly better than just guessing $Y$ with no knowledge of the predictors?  (Round 1: Full vs Null) 
2. How do we know that the addition of radio and the interaction significantly improved our model (Round 2: Full vs Reduced)? 

We'll then attempt to tie this in with the $F$ test that comes in Math 37 in the context of ANOVA and show how this all fits into the framework of multiple regression. 


## Round 1: Fight!

First we would like to develop a statistical method for comparing the performance of a linear model $Y = X \beta + \varepsilon$ vs what we call a "no information" or "null" model. A _null_ model is one in which we try to predict $Y$ without any knowledge of $X$ so that our model looks like 

$$
Y = \beta_0 + \varepsilon. 
$$
where $\beta_0$ is just a constant. In this case, one can show that $\bar{y} = \frac{1}{n} \sum y_i$ is the least squares estimator^[If you are interested in the proof, look at the function $f(\beta_0) = \sum (y_i - \beta_0)^2$ and take the derivative with respect to $\beta_0$. Setting the result equal to 0 will lead to the single critical point $\beta_0 = \bar{y}$ and then you can show it is a minimum using the first or second derivative test.] of $\beta_0$ and the corresponding sum of squared errors would end up being
$$
SST = \sum (y_i - \bar{y})^2.
$$
In contrast, the total variance not explained by a linear model is the sum of squared errors
$$
SSE = \sum (y_i - \hat{y}_i)^2
$$
This is the error that remains after we make our predictions. The $R^2$ statistic is a metric for determining if our linear model is better than the null model: if
$$
R^2 = 1 - SSE/SST = (SST-SSE)/SSE
$$
is "closer to 1", our model is good and if its "closer to 0", our model is shitty. 

The problem is, what do we mean by "close to 0"? In statistics, we like to have a hypothesis test which will tell us if something is "significantly different from 0". A hypothesis test requires a test statistic with a known distribution (eg. $z$, $t$, $\chi^2$). Unfortunately, $R^2$ does not have as nice a density function as the closely related $F$ statistic and hence people often use the latter for hypothesis testing purposes. 

The $F$ statistic is a modified version of $R^2$ where we "average" the sums in both the numerator and denominator before looking at the ratio. In other words, instead of a sum of squared errors, we look at a sort of mean squared error. Life would be too easy if we could just divide everything by $n$, but instead, there's this little fucker called degrees of freedom we need to worry about. 

In linear models, degrees of freedom is the sample size - number of estimated parameters and is the proper thing to use when averaging sums^[A simple algebraic example is as follows. Suppose we have two points $y_1, y_2$ and we look at the sum of squares total
$$
(y_1 - \bar{y})^2 + (y_2-\bar{y})^2
$$ 
Substituting $\bar{y} = (y_1+y_2)/2$ and simplifying reduces this to a single term
$$
(y_1-y_2)^2/2
$$
In essence this means that the sum of squared errors actually involves only one independent term instead of two so if we want to look at the mean sum of squares, we should divide by $2-1$ instead of 2. We lost one degree of freedom because we estimated the true mean with the sample mean $\bar{y}$ leading to less "dimensionality" in our sum (or alternatively, we could say that we projected down one dimension and hence only have $n-1$ dimensions in our space of errors). We loose a dimension for each estimation we make. In linear algebra, this is equivalent to the idea that if you project a vector onto a $d$ dimensional subspace of $R^n$, there are $n-d$ dimensions in the orthogonal space. Since estimating $y$ with $\hat{y} = X \hat{\beta}$ where $\hat{\beta}$ is the least squares esimator of $\beta$ is equivalent to projecting $y$ onto the column space of $X$, we loose $p+1$ dimensions ($p$ predictors + the intercept). But now I am going too far so I'll shut up. ]. That's why when we calculate the sample standard deviation, we divide by $n-1$: we are estimating one parameter (the population mean $\mu$) with a sample statistic ($\bar{x}$). In a linear model, we have $p+1$ parameters so the degrees of freedom is $n-(p+1)$. Thus, we are going to replace the denominator in $R^2$ with 
$$
SSE/(n-(p+1))
$$
a term called the "mean squared error" (even though it is not the same as the MSE we previously discussed). For the numerator
$$
SST-SSE
$$
the degrees of freedom in SST is $n-1$ (we are estimating only one parameter) so the difference in the degrees of freedom is
$$
(n-1) - (n-p-1) = p
$$
Hence, we will replace the numerator with
$$
(SST-SSE)/p
$$
a quantity called the mean squared regression or "treatment". The ratio of these two quantities
$$
F = \frac{(SST-SSE)/p}{SSE/(n-(p+1))}
$$
is an alternative way of measuring the ratio of explained to unexplained variance in our model. We typically call the numerator degrees of freedom "df1" and the denominator "df2$" (that's how they are referred to in R as well). While we don't have time to get too deep into the world of F distributions (also pretty sweetly called Snedecor's F - google it), here is what the density function looks like for the degrees of freedom in the advertising interaction model

```{r}
curve(df(x,3,200-3-1),xlim=c(0,5),xlab="F",ylab="Density")
abline(v=qf(0.95,3,200-2-1),col="red")
```

The line shows the cutoff for the upper 5% of the curve's area (i.e. for p-value < 0.05). Any $F$ values to the right would be statistically significant. Now here is the actual value of $F$ for the advertising interaction model. 

```{r}
yhat=predict(sales_lmINT,Advertising)
y=Advertising$sales
n=length(Advertising$sales)
p=3
SST=sum((y-mean(y))^2)
SSE=sum((y-yhat)^2)
F=((SST-SSE)/p)/(SSE/(n-p-1))
print(F)
```

I think that is a bit to the right of the line which is why the p-value is really, really small. This model is definitely significantly better than the null!

## Round 2, Extra Sums of Squares: Fight!

Now that we have established that, can we also say that the interaction model is significantly better than the TV only model? These types of questions, where we have one set of predictors, but want to add another and see if the model is improved, can also be answered using an $F$-statistic, but where we replace the null model with the reduced model (i.e. the one with fewer variables). To be more concrete, lets call 

$$
Y = \beta_0 + \beta_1 X_1 + \varepsilon
$$
the reduced model and 

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon
$$
the full model. For each, we can compute predicted values $\hat{y}$ and to distinguish them, we'll use the notation $\hat{y}_{red}$ and $\hat{y}_{full}$ to denote the predictions of the two models and $SSE_{red}, SSE_{full}$ to denote the corresponding sums of squared residuals. So, in our example

```{r}
sales_lmReduced=lm(sales~TV,data=Advertising)
sales_lmFull=update(sales_lmReduced,.~.+TV*radio)
y=Advertising$sales
yhat_red=predict(sales_lmReduced,Advertising)
yhat_full=predict(sales_lmFull,Advertising)
SSE_red=sum((y-yhat_red)^2)
SSE_full=sum((y-yhat_full)^2)
```


Now let's make the following replacements in the $F$ statistic
$$
SST \to SSE_{RED}
$$
and 
$$
SSE \to  SSE_{FULL}
$$

For degrees of freedom, df2 will stay the same, but df1 will now be the difference in degrees of freedom
$$
df_{red} - df_{full} = n-p_1-1 - (n-p_2-1) = p_2-p_1
$$
where $p_1, p_2$ are the numbers of variables used in the reduced and full models. In other words, the numerator degrees of freedom is the number of added parameters. Set $q=p_2-p_1$. This leads to the new statistic
$$
F = \frac{(SSE_{red}-SSE_{full})/q}{SSE_{full}/(n-p_2-1)}
$$

In our example, $q = 2$ because we added two extra terms and $p_2 = 3$ because we have three variables ($X_1, X_2, X_1\times X_2$) in the full model. Here is the calculation 

```{r}
F=((SSE_red-SSE_full)/2)/(SSE_full/(200-3-1))
print(F)
```

You can use the anova function in R to compute this directly and also get a p-value for this extra sum of squares test

```{r}
anova(sales_lmReduced,sales_lmFull)
```

The p-value is again really small which means that the interaction model is significantly better than the TV only one.

## Summary: so far

Using the $F$ tests described in this section, one can test the significance of any model against the null model or against models with a reduced number of variables. Both procedures can be encapsulated in the statistic
$$
F = \frac{(SSE_{red}-SSE_{full})/q}{SSE_{full}/(n-p_1-1)}
$$

where the reduced model could be the null model, $q$ is the difference in the number of variables used in the two models, and $p_1$ is the number of parameters in the full model. In R, this is usually implemented using the anova function. 

## Qualitative Variables

To discuss how this is connected to ANOVA, we need to transition a bit from discussing quantitative variables (whose possible values are numbers) to qualitative variables (whose possible values are characters or strings of characters). In the next module, we will discuss qualitative variables as outcomes, but in these notes, we'll focus on their role as predictors in regression models. As an example, we will be using the credit data set, also from the book website

```{r}
url="https://www.statlearning.com/s/Credit.csv"
credit=read_csv(url)
glimpse(credit)

```

In the group work for today (which you may not have done yet if you read these notes in advance - thinking of your Crystal), you tested out the different possible numeric predictors in the dataset and (hopefully) found that three were significant: Income, Rating, and Limit. But what if we want to know how being a student affected balance? Or marital status? Or Region? R doesn't seem to mind if we throw one in

```{r}
balance_lmSTUD=lm(Balance~Student,data=credit)
summary(balance_lmSTUD)
```

Furthermore, something significant and important is going on when we do add the student variable! The goal of these note next section will be to understand what lm is doing when you add in a qualitative predictor.

## Binary Predictors

The situation is easiest to explain when your predictor is binary like student. In this case, we choose one value to serve as the baseline for the model (R does this by alphabetical order so it will choose "No"), associate this value with 0, and the other value with 1. In other words, if we wanted to use a binary predictor in an SLM, we would define
$$
X_1 = \begin{cases}1 & \text{if  Student == "Yes"} \\ 0 & \text{if  Student == "No"} \end{cases}
$$

Then our model is 
$$
Y= \beta_0 + \beta_1 X_1 + \varepsilon 
$$
as before. From this perspective, the coefficient estimate $\hat{\beta}_1$ is then the difference between the average Balance of students vs non-students. Since this came out as about 396 in our model, we conclude that students, on average, had about 396 thousand more on their balance than non-students. 

If you want to combine binary predictors with quantitative predictors, that is fine too. For example, we could add the Student variable into our model from last class.

```{r}

balance_lm=lm(Balance~Income+Limit+Rating+Student,data=credit)
summary(balance_lm)

```

Looks like it helps our model and in fact, the student effect is greater than in the SLM. 

We can also check for interactions between our binary variable and our predictor. Such interactions are often the most common as they result from one group having a fundamentally different joint relationship with the predictors and response than the other. For example, income probably plays a different role in student debt. Let's check by plotting separate lines for both groups

```{r}
credit %>% ggplot(aes(x=Income,y=Balance,col=Student))+
  geom_point(shape=1)+
  geom_smooth(method="lm",se=FALSE)
```

It is possible that the lines are not parallel, but the difference is probably not significant here. Let's check

```{r}
balance_lmINT=update(balance_lm,.~.+Student*Income)
summary(balance_lmINT)
```

Nope, doesn't look like it was significant. I'll leave it to the reader to try the others

## More than two categories

For qualitative predictors with $k \geq 2$ categories, the idea is similar. We choose one category to serve as baseline and then we create $k-1$ binary variables for the other categories of the form
$$
X_j = \begin{cases}1 & \text{if  Category ==} j \\ 0 & \text{if  Category !=} j\end{cases}
$$

Note that the != is R language for "not equal to". The $X_j$ are called indicator or (group) dummy variables. Again, lm is built to handle this kind of shit. For example, suppose we want to throw in the Region variable which has three possible values:

```{r}
table(credit$Region)
```

Then R will choose "East" as the baseline value and create two additional binary predictors for South and West: 


```{r}
balance_lmRegion=update(balance_lm,.~.+Region)
summary(balance_lmRegion)
```

In this case, both coefficients were positive, meaning that overall balance has higher in the South and West after fixing the other variables, however, the differences with East were not significant. The lack of significance is clear in the boxplot. 

```{r}
credit %>% 
  ggplot(aes(x=Region,y=Balance))+
  geom_boxplot(width=0.4,fill="aquamarine")
```

## Tie in with ANOVA and $F$ tests

In Math 37 (and other stats courses), we typically cover a hypothesis test called ANalysis Of VAriance (ANOVA). ANOVA covers the following scenario: Suppose we wish to see if a quantitative outcome $Y$ varies significantly across $k$ groups. Then we wish to test $H_0: \mu_1 = \mu_2 = \cdots =\mu_k$ where $\mu_i$ is the population mean for group $i$. In terms of a statistical learning model, we could think of this as testing between a hierarchy of two models: the null model of 
$$
Y = \beta_1 +\varepsilon
$$
where $\beta_1$ is a constant (overall) mean and 
$$
Y = \beta_1 + \beta_2 X_2 + \cdots \beta_kX_k + \varepsilon
$$
where $X_i$ is the indicator for whether of not the individual is in group $i$ and group 1 is baseline. Here, $\beta_1$ would be the group 1 mean $\mu_1$ and each of the other $\beta$'s would be the difference between $\mu_i$ and $\mu_1$. Thus, we could use the $F$ test mentioned above to test for whether of not $\beta_2 = \beta_3 =\cdots =\beta_k$ are all zero. The full, individual group means model would have $df=n-k$ where $k$ is the number of groups and it has $k-1$ more parameters than the full model so the $F$ is
$$
F = \frac{(SST-SSE)/(k-1)}{SSE/(n-k)}
$$
In Math 37 (and typical ANOVA), the denominator is called the Mean Square Error (basically our MSE except normalized by df) and the numerator is called the Mean Square Treatment because it represents how much better the model with group effects ("treatments") did than baseline. 


